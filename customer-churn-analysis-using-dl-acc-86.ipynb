{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"In this Kaggle notebook, we tackle the practical challenge of predicting customer churn within the banking sector. With a dataset of 10,000 records at hand, our focus is on uncovering insights and patterns that indicate a customer's likelihood of leaving the bank. By utilizing deep learning techniques, we aim to provide a data-driven approach to identifying potential churn and contributing to effective retention strategies.","metadata":{}},{"cell_type":"markdown","source":"For that our job consist of some steps :\n1. Getting The Data\n2. Cleaning Our Data\n3. Data Preproccessing And Feature Engineering\n4. Building Our Neural Network Model \n5. Making Predictions ","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import GridSearchCV\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-27T01:28:56.114228Z","iopub.execute_input":"2023-08-27T01:28:56.114694Z","iopub.status.idle":"2023-08-27T01:29:07.931888Z","shell.execute_reply.started":"2023-08-27T01:28:56.114666Z","shell.execute_reply":"2023-08-27T01:29:07.930475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Getting The Data","metadata":{}},{"cell_type":"markdown","source":"Since Our data is a csv format it would be a great choise to transforme it into a DataFrame using **read_csv** ","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/deep-learning-az-ann/Churn_Modelling.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:07.932955Z","iopub.execute_input":"2023-08-27T01:29:07.933797Z","iopub.status.idle":"2023-08-27T01:29:08.007171Z","shell.execute_reply.started":"2023-08-27T01:29:07.93376Z","shell.execute_reply":"2023-08-27T01:29:08.006036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Statistics","metadata":{}},{"cell_type":"markdown","source":"Now that we got our data , our job is to have a look at it and understand its structure","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:08.008278Z","iopub.execute_input":"2023-08-27T01:29:08.008529Z","iopub.status.idle":"2023-08-27T01:29:08.041311Z","shell.execute_reply.started":"2023-08-27T01:29:08.008505Z","shell.execute_reply":"2023-08-27T01:29:08.040185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:08.044908Z","iopub.execute_input":"2023-08-27T01:29:08.045243Z","iopub.status.idle":"2023-08-27T01:29:08.090908Z","shell.execute_reply.started":"2023-08-27T01:29:08.045216Z","shell.execute_reply":"2023-08-27T01:29:08.08944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(24,18))\naxes = axes.flatten()\nfeatures = [\"Geography\",\"Gender\",\"Tenure\",\"NumOfProducts\",\"HasCrCard\",\"IsActiveMember\"]\nfor i in range(len(features)):\n    sns.countplot(x=features[i],data=data, palette=\"Set2\",ax=axes[i])\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:08.092521Z","iopub.execute_input":"2023-08-27T01:29:08.092852Z","iopub.status.idle":"2023-08-27T01:29:08.959075Z","shell.execute_reply.started":"2023-08-27T01:29:08.092826Z","shell.execute_reply":"2023-08-27T01:29:08.958037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=\"Exited\",data=data, palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:08.960212Z","iopub.execute_input":"2023-08-27T01:29:08.961172Z","iopub.status.idle":"2023-08-27T01:29:09.132178Z","shell.execute_reply.started":"2023-08-27T01:29:08.961117Z","shell.execute_reply":"2023-08-27T01:29:09.130977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the info and the charts plotted we can understand that we have a dataset with **10000 observations** with **0 Missing Values** that contains 13 features/inputs and a single target/output which is **Exited** .\n\nWe Can also notice that we have 3 catagorical features to encode and the rest is all numerical . We will cover that in the cleaning faze of this notebook.\n\nWe Can Conclude from the charts that we have an umbalanced dataset we have 8000 customer that didn't quit and 2000 customer quit and it wouldn't make sens if they were balanced because we would have a huge number of customers that quited the bank .\n\nLooking at the other plots we can see that most of the customers are : from france , males and have a card . Also , we have a balanced Tensure and Activity of Members which explains the quit rate : We have about 5000 unactive members which with time they would quit ","metadata":{}},{"cell_type":"code","source":"print(\"Gender Count :\")\nprint(data['Gender'].value_counts())\nprint(\"\\n\")\nprint(\"Female Quit Rate :\")\nprint(data[data[\"Gender\"]==\"Female\"][\"Exited\"].value_counts())\nprint(\"\\n\")\nprint(\"Male Quit Rate :\")\nprint(data[data[\"Gender\"]==\"Male\"][\"Exited\"].value_counts())","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.1348Z","iopub.execute_input":"2023-08-27T01:29:09.135237Z","iopub.status.idle":"2023-08-27T01:29:09.148708Z","shell.execute_reply.started":"2023-08-27T01:29:09.135206Z","shell.execute_reply":"2023-08-27T01:29:09.14768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=data, x=\"Gender\", hue=\"Exited\", palette=\"pastel\")","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.15016Z","iopub.execute_input":"2023-08-27T01:29:09.150489Z","iopub.status.idle":"2023-08-27T01:29:09.348556Z","shell.execute_reply.started":"2023-08-27T01:29:09.150467Z","shell.execute_reply":"2023-08-27T01:29:09.347081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this Chart we can understand that Females are more likely to quit that Males . Females have a 25% rate to quit where Males have only 16% ","metadata":{}},{"cell_type":"code","source":"print(\"Adresses :\")\nprint(data['Geography'].value_counts())\nprint(\"\\n\")\nl =[\"France\",\"Spain\",\"Germany\"]\nfor x in l:\n    print(f\"{x} Quit Rate :\")\n    print(data[data[\"Geography\"]==x][\"Exited\"].value_counts())\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.35006Z","iopub.execute_input":"2023-08-27T01:29:09.352379Z","iopub.status.idle":"2023-08-27T01:29:09.370959Z","shell.execute_reply.started":"2023-08-27T01:29:09.352344Z","shell.execute_reply":"2023-08-27T01:29:09.369785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=data, x=\"Geography\", hue=\"Exited\", palette=\"pastel\")","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.373088Z","iopub.execute_input":"2023-08-27T01:29:09.373454Z","iopub.status.idle":"2023-08-27T01:29:09.595031Z","shell.execute_reply.started":"2023-08-27T01:29:09.373424Z","shell.execute_reply":"2023-08-27T01:29:09.593655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After having a look at this plot we can conclude that 16% of France and Spain Members quit where Germany got 32% quitting rate which is the highest amoung the 3 countries.","metadata":{}},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"It's Clear that the **RowNumber**,**CustomerId** and **Surname** Columns don't have any predictive power since they are just some general information about a certain client and for that it would make sens to drop them using **drop** method in pandas DataFrames.\nNotice that we are going to set the inplace argument to **True** to apply the modifications to the current data","metadata":{}},{"cell_type":"code","source":"data.drop(columns=[\"RowNumber\",\"CustomerId\",\"Surname\"],inplace=True,axis=1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.596151Z","iopub.execute_input":"2023-08-27T01:29:09.596441Z","iopub.status.idle":"2023-08-27T01:29:09.613498Z","shell.execute_reply.started":"2023-08-27T01:29:09.596412Z","shell.execute_reply":"2023-08-27T01:29:09.611789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we still got 2 Categorical features we will simply use 2 maps to transform them into numerical features","metadata":{}},{"cell_type":"code","source":"map_gen ={\"Male\":1,\"Female\":1}\nmap_geo ={\"France\":0,\"Spain\":1,\"Germany\":2}\ndata[\"Gender\"]=data[\"Gender\"].map(map_gen)\ndata[\"Geography\"]=data[\"Geography\"].map(map_geo)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.615439Z","iopub.execute_input":"2023-08-27T01:29:09.615793Z","iopub.status.idle":"2023-08-27T01:29:09.639201Z","shell.execute_reply.started":"2023-08-27T01:29:09.615766Z","shell.execute_reply":"2023-08-27T01:29:09.638077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"We Will Try and explore our data and create some new features to improve the model efficency","metadata":{}},{"cell_type":"markdown","source":"Let's Start and classify the Credit Score : we can change our **CreditScore** column form a range of scalars into a catagorical and more easier column to interpret from it .\n\nSo we have 5 classes : \n* Exceptional \n* Very Good \n* Good\n* Fair \n* Poor","metadata":{}},{"cell_type":"code","source":"data.loc[ data['CreditScore'] <= 579, 'CreditScore'] = 0\ndata.loc[(data['CreditScore'] >= 580) & (data['CreditScore'] <= 669), 'CreditScore'] = 1\ndata.loc[(data['CreditScore'] >= 670) & (data['CreditScore'] <= 739), 'CreditScore']   = 2\ndata.loc[(data['CreditScore'] >= 740) & (data['CreditScore'] <= 799), 'CreditScore']   = 3\ndata.loc[ data['CreditScore'] >= 800, 'CreditScore'] = 4\ndata[\"CreditScore\"]=data[\"CreditScore\"].astype(int)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.643973Z","iopub.execute_input":"2023-08-27T01:29:09.644337Z","iopub.status.idle":"2023-08-27T01:29:09.664024Z","shell.execute_reply.started":"2023-08-27T01:29:09.64431Z","shell.execute_reply":"2023-08-27T01:29:09.663178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going to do the same work with the Age Colmun","metadata":{}},{"cell_type":"code","source":"data.loc[ data['Age'] <= 32, 'Age'] = 0\ndata.loc[(data['Age'] > 32) & (data['Age'] <= 37), 'Age'] = 1\ndata.loc[(data['Age'] > 37) & (data['Age'] <= 44), 'Age']   = 2\ndata.loc[ data['Age'] > 44, 'Age'] = 3\ndata[\"Age\"]=data[\"Age\"].astype(int)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.66583Z","iopub.execute_input":"2023-08-27T01:29:09.666241Z","iopub.status.idle":"2023-08-27T01:29:09.686003Z","shell.execute_reply.started":"2023-08-27T01:29:09.666206Z","shell.execute_reply":"2023-08-27T01:29:09.684498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=\"Age\",data=data, palette=\"Set2\")","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.687602Z","iopub.execute_input":"2023-08-27T01:29:09.68799Z","iopub.status.idle":"2023-08-27T01:29:09.863324Z","shell.execute_reply.started":"2023-08-27T01:29:09.687958Z","shell.execute_reply":"2023-08-27T01:29:09.86171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Scaling data is vital for optimizing deep learning models. Normalizing input features to a uniform range ensures balanced contributions from all attributes. This process stabilizes gradients. In sum, data scaling enhances model generalization and overall performance. For that, We are going to use **MinMaxScaler** but first we are going to split our data into train , validation and test samples using **train_test_split**","metadata":{}},{"cell_type":"code","source":"feature_matrix=data.drop(\"Exited\",axis=1)\ntarget =data[\"Exited\"]","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.864867Z","iopub.execute_input":"2023-08-27T01:29:09.86514Z","iopub.status.idle":"2023-08-27T01:29:09.871735Z","shell.execute_reply.started":"2023-08-27T01:29:09.865114Z","shell.execute_reply":"2023-08-27T01:29:09.870146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_temp, y_train, y_temp = train_test_split(feature_matrix,target , test_size=0.3, random_state=42,stratify=target)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42,stratify=y_temp)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.873597Z","iopub.execute_input":"2023-08-27T01:29:09.874059Z","iopub.status.idle":"2023-08-27T01:29:09.897603Z","shell.execute_reply.started":"2023-08-27T01:29:09.874024Z","shell.execute_reply":"2023-08-27T01:29:09.896253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.898897Z","iopub.execute_input":"2023-08-27T01:29:09.899272Z","iopub.status.idle":"2023-08-27T01:29:09.908843Z","shell.execute_reply.started":"2023-08-27T01:29:09.899237Z","shell.execute_reply":"2023-08-27T01:29:09.90758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_val=scaler.transform(X_val)\nX_test=scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.910006Z","iopub.execute_input":"2023-08-27T01:29:09.910277Z","iopub.status.idle":"2023-08-27T01:29:09.929099Z","shell.execute_reply.started":"2023-08-27T01:29:09.910252Z","shell.execute_reply":"2023-08-27T01:29:09.927271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Building Our Model","metadata":{}},{"cell_type":"markdown","source":"Now We are done with the preparation of our data we will start with building our Neural Network Model and for that we will use a **sequential api model** found in keras inside the TensorFlow library","metadata":{}},{"cell_type":"markdown","source":"Building a sequential model means designing a linear neural network where layers are stacked in a sequence. Information flows from input to output through these layers, enabling the model to learn patterns and features at increasing levels of complexity. ","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n                                 \n            # The first layers must specify the input shape always\n            tf.keras.layers.Dense(32, activation='relu', input_shape=(10,)),\n            tf.keras.layers.Dense(16, activation='relu'),\n\n            # The last layer usually doesn't have activation function in regression\n            tf.keras.layers.Dense(1)                \n\n])","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:09.931004Z","iopub.execute_input":"2023-08-27T01:29:09.932367Z","iopub.status.idle":"2023-08-27T01:29:10.114834Z","shell.execute_reply.started":"2023-08-27T01:29:09.932323Z","shell.execute_reply":"2023-08-27T01:29:10.113294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now to compile the model we will use **binary_crossentropy** as our loss function because our task is to predict a binary value , and we are going to set our optimize to **adam**","metadata":{}},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:10.116158Z","iopub.execute_input":"2023-08-27T01:29:10.118275Z","iopub.status.idle":"2023-08-27T01:29:10.137072Z","shell.execute_reply.started":"2023-08-27T01:29:10.118247Z","shell.execute_reply":"2023-08-27T01:29:10.135989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nhistory = model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs=50,callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:10.138334Z","iopub.execute_input":"2023-08-27T01:29:10.138595Z","iopub.status.idle":"2023-08-27T01:29:21.264238Z","shell.execute_reply.started":"2023-08-27T01:29:10.138573Z","shell.execute_reply":"2023-08-27T01:29:21.263228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test,y_test)\nprint(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:21.265353Z","iopub.execute_input":"2023-08-27T01:29:21.265681Z","iopub.status.idle":"2023-08-27T01:29:21.387145Z","shell.execute_reply.started":"2023-08-27T01:29:21.265652Z","shell.execute_reply":"2023-08-27T01:29:21.385664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's Great We got a 86% accuracy","metadata":{}},{"cell_type":"markdown","source":"### Accuracy variation","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-27T01:29:21.388244Z","iopub.execute_input":"2023-08-27T01:29:21.388508Z","iopub.status.idle":"2023-08-27T01:29:21.616151Z","shell.execute_reply.started":"2023-08-27T01:29:21.388485Z","shell.execute_reply":"2023-08-27T01:29:21.61495Z"},"trusted":true},"execution_count":null,"outputs":[]}]}